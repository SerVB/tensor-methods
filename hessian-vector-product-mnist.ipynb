{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$Arg \\ min_y [scalarProd(f'(x), y-x) + \\frac{1}{2} \\cdot scalarProd(f''(x)(y-x), y-x) + \\frac{M}{6} ||y-x||^3]$ для логистической регрессии – решаем спуском.\n",
    "\n",
    "$y$ – текущие веса, размерность `[featureCount + 1, clsCount]` (`+ 1` из-за добавления `bias` признака). Заменяем на $W$.\n",
    "\n",
    "$Arg \\ min_W [scalarProd(f'(x), W-x) + \\frac{1}{2} \\cdot scalarProd(f''(x)(W-x), W-x) + \\frac{M}{6} ||W-x||^3]$\n",
    "\n",
    "$x$ – константа с весами, размерность `[featureCount + 1, clsCount]` (`+ 1` из-за добавления `bias` признака). Заменяем на $W_k$.\n",
    "\n",
    "$Arg \\ min_W [scalarProd(f'(W_k), W-W_k) + \\frac{1}{2} \\cdot scalarProd(f''(W_k)(W-W_k), W-W_k) + \\frac{M}{6} ||W-W_k||^3]$\n",
    "\n",
    "$f$ – заменяем на $cost$.\n",
    "\n",
    "$Arg \\ min_W [scalarProd(cost'_W(W_k), W-W_k) + \\frac{1}{2} \\cdot scalarProd(cost''_W(W_k)(W-W_k), W-W_k) + \\frac{M}{6} ||W-W_k||^3]$\n",
    "\n",
    "Меняем $cost''_W(W_k)(W-W_k)$ на $hVecProd$.\n",
    "\n",
    "$Arg \\ min_W [scalarProd(cost'_W(W_k), W-W_k) + \\frac{1}{2} \\cdot scalarProd(hVecProd(cost, W, W-W_k)(W_k), W-W_k) + \\frac{M}{6} ||W-W_k||^3]$\n",
    "\n",
    "Прописываем точный вызов градиента.\n",
    "\n",
    "$Arg \\ min_W [scalarProd(gradient(cost, W)(W_k), W-W_k) + \\frac{1}{2} \\cdot scalarProd(hVecProd(cost, W, W-W_k)(W_k), W-W_k) + \\frac{M}{6} ||W-W_k||^3]$\n",
    "\n",
    "$scalarProd$ трактуем как сумму поэлементных перемножений матриц, норма – корень из суммы квадратов элементов матрицы."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Повычисляем градиенты слагаемых аналитически.\n",
    "\n",
    "Слагаемое 1:\n",
    "\n",
    "$gradient(\\frac{M}{6} || W - W_k||^3) = \\dots$\n",
    "\n",
    "$\\frac{d \\ \\frac{M}{6} || W - W_k||^3}{d \\ W[i]} =$\n",
    "\n",
    "$= \\frac{M}{6} \\frac{d \\ (\\sum^n_{j=1}(W[j] - W_k[j])^2)^{\\frac{3}{2}}}{d \\ W[i]}=$\n",
    "\n",
    "$= \\frac{M}{6} \\frac{3}{2} (\\sum^n_{j=1}(W[j] - W_k[j])^2)^{\\frac{1}{2}} \\frac{d(W[i]-W_k[i])^2}{d \\ W[i]}=$\n",
    "\n",
    "$= \\frac{M}{6} \\frac{3}{2} ||W-W_k|| 2 (W[i]-W_k[i])=$\n",
    "\n",
    "$= \\frac{M}{2} ||W-W_k|| (W[i]-W_k[i])$\n",
    "\n",
    "$\\dots = \\frac{M}{2} ||W-W_k|| (W-W_k)$\n",
    "\n",
    "Слагаемое 2: TODO."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.python.ops.gradients_impl import _hessian_vector_product\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "feature_count = train_images.shape[1] * train_images.shape[2]\n",
    "class_count = len(set(train_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def result_vector(labels, object_ids):\n",
    "    class_ids = labels[object_ids]\n",
    "    answer = np.zeros((len(object_ids), class_count), dtype=np.float64)\n",
    "\n",
    "    for i in range(len(object_ids)):\n",
    "        answer[i][class_ids[i]] = 1\n",
    "    return answer\n",
    "\n",
    "def feature_vector(images, object_ids):\n",
    "    flatten = images[object_ids].reshape((len(object_ids), feature_count))\n",
    "    return np.append(flatten, np.ones((len(object_ids), 1)), axis=1)  # add bias feature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "\n",
    "def get_random_samples_x_y():\n",
    "    object_ids = random.sample(range(len(train_images)), batch_size)\n",
    "    return feature_vector(train_images, object_ids), result_vector(train_labels, object_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:16<00:00, 301.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.22081926  0.36743647  0.02305464 ...  0.50887688 -1.28531052\n",
      "  -0.27542254]]\n"
     ]
    }
   ],
   "source": [
    "def scalar_prod(a, b):\n",
    "    products = tf.math.multiply(a, b)\n",
    "    return tf.reduce_sum(products)\n",
    "\n",
    "maximum_steps = 10\n",
    "step_factor = 0.001\n",
    "maximum_major_steps = 500\n",
    "# maximum_major_steps = 1\n",
    "\n",
    "x = tf.placeholder(tf.float64, [None, feature_count + 1], name=\"x\")\n",
    "y = tf.placeholder(tf.float64, [None, class_count], name=\"y\")\n",
    "\n",
    "M = tf.constant(6, tf.float64, name=\"M\")\n",
    "\n",
    "W_k = tf.Variable(tf.zeros([feature_count + 1, class_count], tf.float64), name=\"W_k\")\n",
    "W = tf.Variable(tf.zeros([feature_count + 1, class_count], tf.float64), name=\"W\")\n",
    "\n",
    "predictions = tf.nn.softmax(tf.matmul(x, W_k))\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions, labels=y)\n",
    "\n",
    "g = tf.gradients(cost, W_k)[0]\n",
    "h_vec_prod = _hessian_vector_product(cost, [W_k], [W - W_k])[0]\n",
    "\n",
    "g_value = tf.Variable(tf.zeros([feature_count + 1, class_count], tf.float64), name=\"g_value\")\n",
    "update_g_value = g_value.assign(g)\n",
    "\n",
    "target_1 = scalar_prod(g_value, W - W_k) + 0.5 * scalar_prod(h_vec_prod, W - W_k)\n",
    "target_1_g = tf.gradients(target_1, W)[0]\n",
    "target_2_g = M / 2 * tf.norm(W - W_k) * (W - W_k)\n",
    "target_g = target_1_g + target_2_g\n",
    "\n",
    "update_W = W.assign_sub(step_factor * target_g)\n",
    "\n",
    "update_W_k = W_k.assign(W)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    with tqdm(total=maximum_major_steps * maximum_steps) as pbar:\n",
    "        for major_steps in range(maximum_major_steps):\n",
    "            features, answers = get_random_samples_x_y()\n",
    "            sess.run(update_g_value, feed_dict={x: features, y: answers})\n",
    "\n",
    "            for steps in range(maximum_steps):\n",
    "                sess.run(update_W, feed_dict={x: features, y: answers})\n",
    "                pbar.update(1)\n",
    "\n",
    "            sess.run(update_W_k)\n",
    "\n",
    "    W_trained = sess.run(W)\n",
    "    print(W_trained)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 873.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct ratio: 0.9194\n"
     ]
    }
   ],
   "source": [
    "def get_random_test_x_label():\n",
    "    object_id = random.randrange(len(test_images))\n",
    "    return feature_vector(test_images, [object_id]), test_labels[object_id]\n",
    "\n",
    "def get_label(predicted):\n",
    "    return np.argmax(predicted)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "predictions_count = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(W_k.assign(W_trained))\n",
    "\n",
    "    for i in tqdm(range(predictions_count)):\n",
    "        features, answer_label = get_random_test_x_label()\n",
    "        predicted = sess.run(predictions, feed_dict={x: features})\n",
    "\n",
    "        predicted_label = get_label(predicted)\n",
    "\n",
    "        if answer_label == predicted_label:\n",
    "            correct += 1\n",
    "\n",
    "print(\"correct ratio:\", correct / predictions_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}